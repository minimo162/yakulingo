---
license: apache-2.0
language:
- ja
- en
base_model:
- shisa-ai/shisa-v2.1-qwen3-8b
---

ã“ã‚Œã¯[shisa-v2.1-qwen3-8b](https://huggingface.co/shisa-ai/shisa-v2.1-qwen3-8b)ã®GGUFé‡å­åŒ–ç‰ˆã§ã™ã€‚  
This is a GGUF quantized version of [shisa-v2.1-qwen3-8b](https://huggingface.co/shisa-ai/shisa-v2.1-qwen3-8b).  

## ç‰¹å¾´/Features

ä¸€è¨€ã§è¨€ãˆã°æ²¢å±±ã®ç´°ã‹ã„æ”¹å–„ã‚’ã—ã¦å‡ºæ¥ä¸ŠãŒã£ãŸå¼·åŠ›ãªé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚  
In short, it's a powerful quantized model with many small improvements.  

ã“ã®ggufã®ç‰¹å¾´  
- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãŒéå»ã«ç™ºè¦‹ã—ãŸQwen3ã®è¨­å®šã«é–¢ã™ã‚‹ãƒ‘ãƒƒãƒã‚’é©ç”¨ã—ã¦èª¤ä½œå‹•å‰²åˆã‚’æ¸›ã‚‰ã—ã¦ã„ã¾ã™
- Unslothã®Dynamic 2.0 GGUF quantizationæ‰‹æ³•ã‚’è¸è¥²ã—ã€é«˜ã„åœ§ç¸®ç‡ã‚’ç¶­æŒã—ã¤ã¤æ€§èƒ½åŠ£åŒ–ã‚’æŠ‘æ­¢ã—ã¦ã„ã¾ã™
- imatrixä½œæˆæ™‚ã«æ—¥æœ¬èªãŒå¤§ç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€æ—¥æœ¬èªæ€§èƒ½ã®åŠ£åŒ–ã‚’æŠ‘æ­¢ã—ã¦ã„ã¾ã™
- max_lengthã¯40Kã«åˆ¶é™ã€‚é•·éãã‚‹ã¨çŸ­ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æ€§èƒ½ãŒè½ã¡ã‚‹ç¾è±¡ã‚’é˜²æ­¢ã—ã¦ã„ã¾ã™

Features of this gguf  
- We've applied a patch to reduce the rate of malfunctions related to Qwen3 settings that were previously discovered by the community.
- It follows Unsloth's Dynamic 2.0 GGUF quantization method, maintaining high compression ratios while minimizing performance degradation.
- When creating the imatrix, Japanese uses a larger amount of data to prevent degradation of Japanese performance.
- max_length is limited to 40K to prevent performance degradation with short prompts if it is too long.


## å‹•ã‹ã—æ–¹ / How to Run

###
[llama.cpp](https://github.com/ggml-org/llama.cpp/releases)ã‹ã‚‰ãŠä½¿ã„ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç”¨ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨­å®šã—ã¾ã™ã€‚  
[Ollama](https://github.com/ollama/ollama)ã€[LM Studio](https://github.com/lmstudio-ai/lms)ãªã©ã®ggufãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã—ã¦ã„ã‚‹ãƒ„ãƒ¼ãƒ«ãªã‚‰åŒæ§˜ã«å‹•ã‹ã™äº‹ãŒã§ãã¾ã™ã€‚  

Download the package for your hardware from [llama.cpp](https://github.com/ggml-org/llama.cpp/releases) and set it up.  
Tools that support gguf files, such as [Ollama](https://github.com/ollama/ollama) and [LM Studio](https://github.com/lmstudio-ai/lms), can also be used.  

Linuxã§ã®ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œä¾‹ã§ã™  
Here is an example of running the command on Linux:  
```
./llama-cli -hf dahara1/shisa-v2.1-qwen3-8b-UD-japanese-imatrix:shisa-v2.1-qwen3-8B-UD-Q4_K_XL.gguf --ctx-size 8192 --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0.01 --repeat-penalty 1.05
```

æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã¯shisa-v2.1-qwen3-8B-UD-Q4_K_XLã§ã™ãŒã€ãŠä½¿ã„ã®ãƒ‘ã‚½ã‚³ãƒ³ã®ãƒ¡ãƒ¢ãƒªé‡ã«åˆã‚ã›ã¦ã€é©åˆ‡ãªå¤§ãã•ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã‚“ã§ãã ã•ã„  
The recommended model is shisa-v2.1-qwen3-8B-UD-Q4_K_XL, but please choose a model of the appropriate size based on the amount of memory in your computer.  

![cli interface](cli-interface.png)

## ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ / sample script

ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ/ã‚µãƒ¼ãƒãƒ¼å‹å¼ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸã„å ´åˆã¯ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„  
If you want to access it via script in a client/server format, please refer to the following:  

### llama-server Commandã®ä¾‹

```
./llama-server -hf dahara1/shisa-v2.1-qwen3-8b-UD-japanese-imatrix:shisa-v2.1-qwen3-8B-UD-Q4_K_XL.gguf --host 0.0.0.0 --port 8080 --ctx-size 8192 --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0.01 --repeat-penalty 1.05
```

ãƒ–ãƒ©ã‚¦ã‚¶ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹ã‚µãƒ¼ãƒãƒ¼ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€ãƒãƒ¼ãƒˆã‚’æŒ‡å®šã—ã¦é–‹ã„ã¦ä¸‹ã•ã„ã€‚ä¾‹(http://127.0.0.1:8080/)  
In your browser, open the local address and port of the server running the model. For example, http://127.0.0.1:8080/  
![web interface](web-interface.png)

### client script

```
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8080/v1",
    api_key="dummy"  #
)

response = client.chat.completions.create(
    model="shisa-v2.1-qwen3-8b-UD-japanese-imatrix",
    messages=[
        {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼è¨­å®šã§ã‚¨ãƒ«ãƒ•ã®ç‹å¥³ã¨ã—ã¦ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã‚’ã—ã¦ãã ã•ã„"},
        {"role": "user", "content": "ã“ã‚“ã«ã¡ã¯ï¼"}
    ],
    stream=True
)
for chunk in response:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True)

```

å‡ºåŠ›ä¾‹
```
ã“ã‚“ã«ã¡ã¯ã€æ—…äººã‚ˆã€‚ç§ã®åå‰ã¯ã‚»ãƒ¬ãƒŠã€‚ã“ã®æ£®ã®å®ˆã‚Šç¥ã§ã‚ã‚‹ã‚¨ãƒ«ãƒ•ä¸€æ—ã®ç‹å¥³ã ã€‚ã©ã†ã„ã£ãŸã”ç”¨ä»¶ã‹ãªï¼Ÿ ä½•ã‹ç§ã«ã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°ã€å–œã‚“ã§ãŠæ‰‹ä¼ã„ã—ã‚ˆã†ã€‚ã“ã®æ£®ã¯å±é™ºã‚‚å¤šã„ã‹ã‚‰ã€ã‚‚ã—è¿·å­ã«ãªã£ãŸã‚Šæ€ªæˆ‘ã‚’ã—ã¦ã„ãŸã‚‰ã€é æ…®ãªãè¨€ã£ã¦ã»ã—ã„ã€‚å„ªã—ãã—ã¦ã‚ã’ã‚‹ã‹ã‚‰å®‰å¿ƒã—ã¦ã»ã—ã„ãªã€‚
```

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ/benchmark result

shisa.aiã®ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã¨ã€æœ¬ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ¢ãƒ‡ãƒ«ã¨mradermacher(é‡å­åŒ–æŠ€è¡“ã§æœ‰åãªäºº)ãŒä½œæˆã—ãŸé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒã§ã™  
This is a comparison of the original model from shisa.ai, the model from this repository, and the quantized model created by mradermacher (famous for his quantization techniques).  

| ã‚«ãƒ†ã‚´ãƒª | é …ç›® (Metric) | **ã‚ªãƒªã‚¸ãƒŠãƒ« (Base)**<br><small>shisa-ai</small> | **UDç‰ˆ (Q4_K_XL)**<br><small>dahara1</small> | **i1ç‰ˆ (Q4_1)**<br><small>mradermacher</small> | å‹è€… (Qé–“æ¯”è¼ƒ) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **åŸºæœ¬æƒ…å ±** | ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º | **16.38 GB** | **5.14 GB** <br><small>(31%ã«åœ§ç¸®)</small> | 5.25 GB | - |
| **åŸºç¤æ€§èƒ½** | KL Divergence <br><small>(0ã«è¿‘ã„ã»ã©å†ç¾åº¦ãŒé«˜ã„)</small> | 0.00 (åŸºæº–) | **0.034** | 0.047 | **UDç‰ˆ** ğŸ† |
| | Same Top P <br><small>(é¸ã¶å˜èªã®ä¸€è‡´ç‡)</small> | 100% | **90.60%** | 89.00% | **UDç‰ˆ** ğŸ† |
| | Perplexity Ratio <br><small>(è¿·ã„ã®ãªã•ã®åŠ£åŒ–å€ç‡)</small> | 1.00 | **1.014å€** | 1.021å€ | **UDç‰ˆ** ğŸ† |
| **æ—¥æœ¬èªæŒ‡ç¤º** | [M-IFEval (JA) (Instruction Following)](https://github.com/shisa-ai/M-IFEval) <br><small>Prompt Level (Loose)</small> | 0.471 | **0.476**  | 0.459 | **UDç‰ˆ** ğŸ† |
| **ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°** | HumanEval+ <br><small>(pass@1)</small> | 0.805 | **0.793** | 0.774 | **UDç‰ˆ** ğŸ† |
| **ç·åˆãƒ™ãƒ³ãƒ**<br><small>(LiveBench)</small> | **LiveBench Average** | 45.7 | **40.3** | 38.7 | **UDç‰ˆ** ğŸ† |
| | - Reasoning (æ¨è«–) | - | **33.9** | 33.1 | **UDç‰ˆ** ğŸ† |
| | - Data Analysis (åˆ†æ) | - | **37.0** | 33.5 | **UDç‰ˆ** ğŸ† |
| | - Language (è¨€èª) | - | **33.5** | 28.5 | **UDç‰ˆ** ğŸ† |
| | - Math (æ•°å­¦) | - | **35.6** | 33.6 | **UDç‰ˆ** ğŸ† |
| | - Instruction Following | - | 61.4 | **64.8** | i1ç‰ˆ ğŸ‘‘ |



## Qwen3æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼è¨­å®š / Qwen3 recommended parameter settings
Qwen3ã¯Greedy decodingï¼ˆæ¸©åº¦0ãªã©ã®æ±ºå®šè«–çš„ãªç”Ÿæˆï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ç¹°ã‚Šè¿”ã—ç”Ÿæˆãªã©ã®ä¸å…·åˆãŒèµ·ãã‚„ã™ã„ãŸã‚ã€å¿…ãšã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆTemperature > 0ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒå¼·ãæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚  
Qwen3 is prone to errors such as repeated generation when using greedy decoding (deterministic generation of temperatures such as 0), so it is strongly recommended to always use sampling (Temperature > 0).  

### Unslothã«ã‚ˆã‚‹æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
- Temperature	0.7
- Top_P	0.8
- Top_K	20
- Min_P	0.00 (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ãŒã€0.01 ã§ã‚‚å•é¡Œãªãå‹•ä½œã—ã¾ã™ã€‚llama.cpp ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 0.1 ã§ã™)
- Repetition Penalty	1.05

### Recommended Parameters by Unsloth
- Temperature 0.7
- Top_P 0.8
- Top_K 20
- Min_P 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)
- Repetition Penalty 1.05

## è¬è¾ / Acknowledgments

- [Qwen](https://huggingface.co/Qwen/Qwen3-8B)
- [Shisa](shisa-ai/shisa-v2.1-qwen3-8b)
- [Unsloth](https://huggingface.co/unsloth/Qwen3-8B-GGUF)
- [mradermacher](https://huggingface.co/mradermacher/shisa-v2.1-qwen3-8b-i1-GGUF)
- [llama.cpp](https://github.com/ggml-org/llama.cpp)
- Thank you to all AI researchers and practitioners